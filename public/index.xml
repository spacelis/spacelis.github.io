<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>0x4C</title>
    <link>https://spacelis.github.io/index.xml</link>
    <description>Recent content on 0x4C</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Mar 2017 10:49:31 +0000</lastBuildDate>
    <atom:link href="https://spacelis.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Thoughts on data protection: Hashing!</title>
      <link>https://spacelis.github.io/post/thoughts-on-data-protection/</link>
      <pubDate>Mon, 13 Mar 2017 10:49:31 +0000</pubDate>
      
      <guid>https://spacelis.github.io/post/thoughts-on-data-protection/</guid>
      <description>&lt;p&gt;TL;DL If you need to share a (key, value) dataset without sharing the whole key set, hash all the keys and give out only (hashed key, value) dataset.
&lt;/p&gt;

&lt;p&gt;Riddles, secrets exchanging, encryption and so on are always fascinating to fiddle with.
Modern cryptography is based on mathematical theories which guarantees the hardness of breaking.
This means circumventing a cryptographic algorithm is about the same as tackling a corresponding hard mathematical problem.
These building blocks can then be used to design communication protocols enabling secure way of sharing sensitive data.&lt;/p&gt;

&lt;p&gt;Sometimes giving out the whole data set may be too &lt;em&gt;generous&lt;/em&gt; in the sense that you have no control on what information your &lt;em&gt;collaborators&lt;/em&gt; would extract from the data.
You may want to limit your data to a certain type of use or limit the operations that can be applied to the data.
This poses a problem of &lt;strong&gt;information exposure controlling&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;It is a very big topic and can not be fully addressed in this blog.
Here only provides one angle approaching the problem.
Let&amp;rsquo;s consider a scenario that we want to share some data with our collaborators, allowing them to join their data with ours.
We can model the data as a lookup table, that is, a set of key-value pairs.
However, we do not want them to discover keys from our data other than the ones they already have for joining.
That is, they can &lt;strong&gt;only&lt;/strong&gt; extract the values if they already have the key.
We may also want to limit their lookup speed to make sure they would not have an quick scan through the data.
How can we make sure of it?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Similar things must have been popped up before and may already been put into practice.
It would be much appreciated if I can be pointed to some resources.
Here is my thoughts on it.&lt;/p&gt;

&lt;h2 id=&#34;an-approach&#34;&gt;An Approach&lt;/h2&gt;

&lt;p&gt;To start, we need to clarify the goals.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Discovering the key set&lt;/em&gt;: Attackers can recover the keys from the data with no external resources.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Lookup speed limits&lt;/em&gt;: Attackers are not being able to do lookup faster than we processing the data given the same computation power.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The second goal can be strengthened to that attackers&amp;rsquo; machine can be a finite times faster than ours and we can still make sure they are not able to lookup faster than the speed of processing the data.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s review how a data set of key-value pairs is used.
Users will query the data with a known key and may (not) find a match in the data as well as its associated value.
That is they present a piece of information, and the data give them another piece of information matching theirs.
Note that they have the first piece of information and they find it outside the data.
This means we do not need to provide the data sets with the actual keys since they already have it.
But we still need to keep its capability of matching keys.
That is, we allow them to find a match but they can not know whether the data have a key unless they know the key already.
Sounds familiar?
Yes, hashed password!&lt;/p&gt;

&lt;h2 id=&#34;hashing&#34;&gt;Hashing&lt;/h2&gt;

&lt;p&gt;Hashing has been widely used in password protection against database dumping.
The basic idea is to convert the plain text password into a sequence of bytes and the process can not inverse.
Given $x$, it is easy to compute $y=f_h(x)$, but given $y$ it is very hard to find $x$ and the best way may be random guess $x$ and verify whether it can produce the $y$ via the hashing function $f_h$.
Hashing functions make it possible for a system to verify whether a password is valid without store the actual password.
(Note: One should always use hashing function with salt when design a system dealing with passwords. Check &lt;a href=&#34;https://en.wikipedia.org/wiki/Salt_(cryptography)&#34;&gt;Wiki Page&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&#34;hashing-key-set&#34;&gt;Hashing Key Set&lt;/h2&gt;

&lt;p&gt;This is what we want in the aforementioned problem, matching without storing the original keys.
Thus, we process our data by replacing all the keys with their hash values.
$$ (k, v) \Rightarrow (f_h(k), v) $$
When users query the data, they are required to do the same to their keys before finding a match in the processed data.
Hence, the processed data can handle matching without storing the actual things!&lt;/p&gt;

&lt;p&gt;With this processed data $(f_h(k), v)$, users can perform their query as they have the original data.
For example, they can find a $v$ by a $k$, or join their list of $k$s with the data.
All they need to do is to compute $f_h(k)$ first and this would only increase the computation time a little bit.&lt;/p&gt;

&lt;p&gt;Hashing can prevent attackers to obtain the original keys by only looking at its hash value.
But attackers can still get help from big dictionaries, containing possible sequences of characters along with their hashing values.
They can look up the hash values from the target data in the dictionary and the matched ones will tell them what the original key is for those hash values.
To strengthen the protection, we need to use different hashing functions for our keys in one data set.
An easy way of doing that is to add random salt to the keys before hashing: $f_{sh}(k) = f_h(ks)$, where $ks$ means concatenating $k$ with $s$.
Every hash value is then produced by a unique hash function.
Thus a dictionary attack is nearly impossible.
Of course, the uniqueness of hash functions depends on how the salt is generated.&lt;/p&gt;

&lt;h2 id=&#34;retrial-speed-limit&#34;&gt;Retrial Speed Limit&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the second goal we want to achieve, &lt;em&gt;i.e.&lt;/em&gt;, limiting the speed of query.
I started by looking for an encryption algorithm that having significant speed difference between encrypting and decrypting.
But later I realized that such algorithms would not be so reliable in limiting the speed, since algorithms are not designed to have a different speed and the difference is unlikely to be adjustable easily.
Please leave a comment if you know any encryption algorithm that can do the job.&lt;/p&gt;

&lt;p&gt;We may not find an algorithm to slow down the decryption, but we can slow down the description by making it harder for retrieve the decryption key.
Suppose we have a symmetric encryption algorithm that take a password $p$ to encrypt a piece of information $v$ to $g_e(v, p)$.
For each $v$ in the dataset, we choose a $p$ randomly from a fixed set of passwords, for example $[0, 255]$.
Then we apply encryption and throw away $p$ immediately after encrypting them.
Now a user of the data has to guess each $p$ to decrypt the piece of information $g_e(v, p)$.
Since the password is guaranteed in the fixed range, users will eventually find the password and decrypt the piece of information via, for example, random guess.
With this technique, we can dial the speed limit down by increase the size of password set and it would not increase the processing speed.
Of course this is based on the assumption that users&amp;rsquo; computational power is similar to ours.&lt;/p&gt;

&lt;p&gt;We can further slow down the decrypting process by generating passwords from random numbers via hashing.
Given a random number $e$, we generate the password by $$p = \underbrace{\mathrm{hash}( \mathrm{hash}(&amp;hellip; \mathrm{hash}}_{n} (e)&amp;hellip;)).$$
This gives us a strong password and slow down the retrieval speed even further.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Information Exposure Controlling is useful in the way that the owner of the data can make sure their data is used in a proper way.
Hashing functions can be used in different part of the problem.
This is just a showcase of the problem and what we can do about it.
Let me know if you have any thoughts on it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CSV-Loader</title>
      <link>https://spacelis.github.io/post/csv-loader/</link>
      <pubDate>Fri, 09 Dec 2016 09:02:19 +0000</pubDate>
      
      <guid>https://spacelis.github.io/post/csv-loader/</guid>
      <description>

&lt;p&gt;My work requires me importing a variety tables from CSV to DBMS.
However, importing CSV is not always an easy and documentable task, as every DBMS has its own way of doing the job.
There are different tools, be it GUI or CUI, that can help the process but there lacks a tool generally available for the task.
I needed a tool for importing CSV tables and I took this opportunity to make a general tool.&lt;/p&gt;

&lt;h1 id=&#34;csv-loader&#34;&gt;CSV-Loader&lt;/h1&gt;

&lt;p&gt;CSV-Loader uses SQLAlchemy for generalize the DB operating and messytables for schema detecting.
The tool is composed of several commands, one is for generating schemas from arbitrary CSV files.
Another is for uploading the data according to the schema generated.
The third is for building single column indices for all columns on a table.&lt;/p&gt;

&lt;h1 id=&#34;schema-as-documentation&#34;&gt;Schema As Documentation&lt;/h1&gt;

&lt;p&gt;The schema generated from the CSV is an intermediate step for importing the CSV files.
It gives users a chance to review the schema before importing millions of rows for a few hours.
Sometimes, simple data type conversion or transformation is needed during the importing.
Thus users can edit the schema file to integrate the conversion or transformation into the importing processes.
In these cases, the schema file can be archived for data documentation, so that when there is similar data required to be imported, the same processes can be reused to ensure the consistance.&lt;/p&gt;

&lt;h1 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h1&gt;

&lt;p&gt;For CSV-Loader, &lt;code&gt;messytables&lt;/code&gt; is used for detecting the types and names of columns in a CSV file.
Accordingly, the schema is generated in pure Python module.
The schema generating tool will also try to convert column names into a format that will comply with the naming rules in the DB.
When column names are missing in the CSV file, users are required to edit the schema file to assign a name to each column.
Otherwise, the tool may fail importing data into the DB.
This will be improved in the future release.
For example, some random name will be generated to enable importing before naming.&lt;/p&gt;

&lt;p&gt;Currently, enabled by &lt;code&gt;SQLAlchemy&lt;/code&gt;, CSV-Loader supports four different RDBMS, that is, SQLite, MySQL, PostgreSQL, MSSQL.
Since only a small set of language is used in the importing process, a later version of CSV-Loader may integrate non-SQL DB importing.
Hopefull, it can help users that requires different DBMS in their data analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BakMan: A file oriented backup management tool.</title>
      <link>https://spacelis.github.io/post/bakman/</link>
      <pubDate>Wed, 02 Nov 2016 22:17:59 +0000</pubDate>
      
      <guid>https://spacelis.github.io/post/bakman/</guid>
      <description>

&lt;p&gt;If you ever had tons of backup files which quickly fill up all your disk space, this tool might be useful to you.
Bakman is a BAcKup MANagement tool for automatically tracing unneeded backup files and generating management scripts.
The core idea is to filtering backup files by a set of customizable rules.
The rule file will define which files in the piles should be removed or stashed somewhere else.&lt;/p&gt;

&lt;h1 id=&#34;bakman&#34;&gt;BakMan&lt;/h1&gt;

&lt;p&gt;This project was originate from my needs to manage the backup files of a website.
The website provides access to quite some datasets which took a lot of place to backup at daily frequency.
It triggered me to thing about the general way of dealing with backup files when the disk was incidently filled up and got the website down.
Therefore I have to delete some backup files to release disk space for running the website.
However, deleting all the backup files up to a certain date is kind of simple and effective way but you may loose the trace and may cause you in trouble if you are not frequently managing the website.
So it might be better to have a tool that allows you to use more complex logic for managing your backups.
For example, you may want to keep all the backups within 7 days and one per week within 3 months and one per month for those older than 3 months.
This logic will greatly reduce the number of backups you need to keep and still give you the ability of tracing the changes since the very beginning.
The downside is that I need to write the tool as I could not find a suitable ones on the Internet.
Maybe I am just curious about whether I can build one myself and there you go BakMan.&lt;/p&gt;

&lt;h1 id=&#34;script-generating-vs-action-immediately&#34;&gt;Script Generating VS action immediately&lt;/h1&gt;

&lt;p&gt;BakMan accepts a rule file defining how the backup files should be separated, that is, those that need to be deal with and those that should be left untouched.
Based on the rules, BakMan will generate a script by feeding those that need to be dealt with into a template.
For example, if you use &lt;code&gt;rm -rf {0}&lt;/code&gt;, then the script will try to remove all the unwanted backup files.
If you use &lt;code&gt;trash {0}&lt;/code&gt;, then the script will put unwanted backup files in the trash can.
An alternative design could be carry out actions immediately without generating a script for user to run.
But making dangeous actions as default is never a good idea.
So giving users a chance to inspect the actual actions that are going to carry out will make sure they are not suprised.
Of course users can still automate the task by piping the output script to bash.
This little extra effort and the delibration asking users to think twice will reduce the chance of going to a disaster.
Similar design can be found every where from querying user when &lt;code&gt;rm&lt;/code&gt; a file to requesting user to type repository name when deleting it on Github.
It also allows trial run with no harm to the system.&lt;/p&gt;

&lt;h1 id=&#34;the-rule-system&#34;&gt;The Rule System&lt;/h1&gt;

&lt;p&gt;The core of BakMan is the rule system, which make it a nice tool for complex backup retaining strategy.
The rule system is based on group-aware marking.
Simple as it is, it marks files based on its role in a group.
The group can be defined by a period of time, for example, a month or a week.
A role in a group can be the first one in the group or the last one.
The markings from different group can either be merged in a conjunction way or a disjunction way.&lt;/p&gt;

&lt;p&gt;The rules are organized in a tree structure as different level of mark merging may be required.
In BakMan, the rule file can be written in YAML or JSON.
A parser will translate it into a tree structure of makers.
This marker will be applied to the files that need to be managed.&lt;/p&gt;

&lt;h1 id=&#34;an-extensible-tool&#34;&gt;An Extensible Tool&lt;/h1&gt;

&lt;p&gt;Along with developing the rule parser, quite some effort was put into the extensibility of the parsers.
The parser is integrated to the markers which a metaclass is created to allow implicit registering of new markers.
Following the pattern, one can easily create a new marker and it will automatically be integrated into the rule system.&lt;/p&gt;

&lt;h1 id=&#34;last-words&#34;&gt;Last Words&lt;/h1&gt;

&lt;p&gt;This is a tool purely for my own interests.
I hope it can become useful for some of you.
Please let me know you if it helps you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World From Me</title>
      <link>https://spacelis.github.io/post/hello-world/</link>
      <pubDate>Thu, 27 Oct 2016 22:38:35 +0100</pubDate>
      
      <guid>https://spacelis.github.io/post/hello-world/</guid>
      <description>&lt;p&gt;I am fascinated in many things, like programming, physics, mathematics, music.
These all have intrinsic patterns that render the beauty of the master piece.
I really enjoy finding the patterns whenever possible.
&lt;/p&gt;

&lt;h2 id=&#34;hello-from-my-childhood&#34;&gt;Hello From My Childhood&lt;/h2&gt;

&lt;p&gt;In my youth time, I am impressed by the huge numbers about stars and the universe.
Everything is so big, totally out of imagination.
I am more impressed that even things of that big can be described in a small little book.
A few statement can make a big prediction on how the stars and the universe would go and evolve.
I can not stop reading and thinking about the mechanisms behind them.&lt;/p&gt;

&lt;p&gt;Later, my interests into the universe generally applied to physics when I was in school.
It describes things and makes predictions of their transiting between states.
I started to like learning the details of a how and why rather than accepting it can work.
Learning physics give me accurate thinking.
Though I admit, there is always a point to stop asking why as the beyond is an imaginary world.
That is my very preliminary taste of philosophy.&lt;/p&gt;

&lt;h2 id=&#34;hello-to-programming&#34;&gt;Hello to Programming&lt;/h2&gt;

&lt;p&gt;Describing things precisely and controlling things is more fun.
That was the start of my programming life.
I write my first BASIC program to move icons around.
The crappy machine I used made me very frustrated because it can not run a program longer than the screen.
Until later I found I could make it work if I consequently input the code without editing back.
My serious programming life started when I chose computer science as my major in the university.
Though I sat through a lot of courses, I think reading and coding myself taught me much more.
Of course, the lectures would give more systematic reviews of the topics, where you learn about alternatives.&lt;/p&gt;

&lt;p&gt;I love programming in general, because you can make things happen with your finger tips.
That is not the whole story.
I think I love programming is because it describe how things work in a very subtle way, as you cannot skip steps in describing a thing or an algorithm.
You have to make sure every details right for a piece of code work and you can hardly cheat on that.
Of course, programming languages and tools hide a lot details to make sure your are not bored with those details.
But I believe if you program for some time, you will eventually learn the details as it determines the behaviours of the thing you make.&lt;/p&gt;

&lt;p&gt;I started programming with Pascal/Delphi.
I went to programming contests and learn to write basic algorithms in Pascal, as it was one of the designated language for contests at the time.
Following that, I learned Delphi when I tried to program on Windows machines.
I don&amp;rsquo;t think I formally learned C/C++, but I enjoy reading &lt;a href=&#34;http://www.stroustrup.com/dne.html&#34;&gt;&lt;em&gt;The Design and Evolution of C++&lt;/em&gt;&lt;/a&gt; by &lt;em&gt;Bjarne Stroustrup&lt;/em&gt;.
It taught me a lot about how a programming language becomes that way and what actually happen when we write code in OOP.
Java is a language I put quite some time into.
Programming a website with JSP is quite interesting experience.
A complex system with different technology combining together is really a challenge.
Every click on a web page involves a lot of parts moving co-ordinately.
Query information from Javascripts, getting requests handled in logic written in Java, retrieving information from databases in SQL, rendering pages in JSP and so on can make a bug buried deeply in the stack.
The only universal general debugger is &lt;em&gt;print&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;hello-to-python-world&#34;&gt;Hello to Python World&lt;/h2&gt;

&lt;p&gt;I learned Python when it started to prevail.
The simplicity made me feel like sitting in breeze.
The forced indentation in the code was really broaden of my mind in how a programming language can help you build a good habit.
Python then became my major language during my PhD.
I ran Python programs on my laptop, on the server and make them scale.
Jupyter (a.k.a IPython Notebook) is my friend which I need when I want to explore some data and plot them.
Some of my favourite packages include but limit to Matplotlib, Pandas, Numpy, Seaborn, Requests, Click.
I also did some personal exploring project like &lt;a href=&#34;https://github.com/spacelis/lispy&#34;&gt;lispy&lt;/a&gt; in which I tried to pounder how a lisp program can be built from Python.
Anther one is &lt;a href=&#34;https://github.com/spacelis/lazylist&#34;&gt;lazylist&lt;/a&gt; which is my trial implementation of infinite list which lazy evaluation in Python.
These projects are not meant to become ones at production level.
They were purely from my curiosity into what a programming language really is.
Turning completeness is great, it can not promise a programming language&amp;rsquo;s expressiveness.
But you can get the feeling of the good and the evil parts of a programming language when you implement some interesting constructs with them.&lt;/p&gt;

&lt;h2 id=&#34;hello-to-scala&#34;&gt;Hello to Scala&lt;/h2&gt;

&lt;p&gt;I am also interested in Scala.
The interests begins when I want to learn some function programming.
I know about Haskell because of my colleague as well as Scala.
The courses on Coursera really inspired me to think about &lt;em&gt;descriptions&lt;/em&gt; instead of &lt;em&gt;commands&lt;/em&gt;.
Scala also has powerful type system and can let you program without much burden in boilerplates.
Many syntactic surges may the code more readable and implicit conversion is really a save of many simple type casting.
Mintsearch is one interesting project built with Scala.
It extensibility can only be that easy with Scala&amp;rsquo;s traits.
Though I believe Python can do similar things as easy as Scala, it can not give you type checking and all other benefit from it.&lt;/p&gt;

&lt;h2 id=&#34;hello-to-a-production-project&#34;&gt;Hello to a production project&lt;/h2&gt;

&lt;p&gt;CDRC Data website is my first production project which is a data portal providing information and downloads of the data we hold in CDRC.
It is based on the open data portal platform called &lt;a href=&#34;http://ckan.org&#34;&gt;CKAN&lt;/a&gt;.
It provides a very extensible plugin system.
So I can avoid much of direct editing its codebase and put all functionality in a plugin called &lt;em&gt;ckanext-cdrc&lt;/em&gt;.
However, it is still difficult if one wants to change behaviours of some deep parts.
For example, we need a two-stage reviewing procedure for data sets when they come into the website.&lt;/p&gt;

&lt;p&gt;Deploying is also another challenging task, as the website relies on several components including a uwsgi running CKAN with plugins, a Solr server for indexing and searching, a PostgreSQL Database for persistence, another uwsgi running datapusher, a nginx for reverseproxy, a memcached for caching SQL queries results.
Making sure all these co-operate with each other is complex, especially when initializing them or updating the code.
Docker is really a nice tool to put things together, I mean, literally.
Contrast to the recommendation of keeping containers small, fat images can help you press all complex configurations into a single vinyl that plays well every where.&lt;/p&gt;

&lt;p&gt;Developing a production project is really a balance walk and many decisions have to evaluate and re-evaluate during the process.&lt;/p&gt;

&lt;h2 id=&#34;finale&#34;&gt;Finale&lt;/h2&gt;

&lt;p&gt;I am enthusiasm about programming and believe programming can guide me to understand how the world really works.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Science is what we understand well enough to explain to a computer. &amp;ndash; D. Knuth&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>